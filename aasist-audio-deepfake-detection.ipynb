{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6499184,"sourceType":"datasetVersion","datasetId":3756644}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages (using latest versions)\n!pip install torchaudio librosa tensorboardX matplotlib soundfile tqdm pyyaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:38.900356Z","iopub.execute_input":"2025-04-05T14:06:38.900683Z","iopub.status.idle":"2025-04-05T14:06:43.319527Z","shell.execute_reply.started":"2025-04-05T14:06:38.900633Z","shell.execute_reply":"2025-04-05T14:06:43.318464Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\nCollecting tensorboardX\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.2)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\nDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboardX\nSuccessfully installed tensorboardX-2.6.2.2\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport torch\nimport torchaudio\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport yaml\nimport random\nimport time\nimport librosa\nimport soundfile as sf\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\nfrom tensorboardX import SummaryWriter\nfrom collections import OrderedDict\nimport psutil\nimport gc\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:43.320729Z","iopub.execute_input":"2025-04-05T14:06:43.320969Z","iopub.status.idle":"2025-04-05T14:06:59.185100Z","shell.execute_reply.started":"2025-04-05T14:06:43.320949Z","shell.execute_reply":"2025-04-05T14:06:59.184458Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Check GPU availability and set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.186566Z","iopub.execute_input":"2025-04-05T14:06:59.187041Z","iopub.status.idle":"2025-04-05T14:06:59.231571Z","shell.execute_reply.started":"2025-04-05T14:06:59.187018Z","shell.execute_reply":"2025-04-05T14:06:59.230740Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Set random seed for reproducibility\ndef setup_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    # Keep deterministic mode for reproducibility of results\n    torch.backends.cudnn.deterministic = True\n    # Enable benchmark mode for optimized performance with fixed input sizes\n    torch.backends.cudnn.benchmark = True\n\nsetup_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.232888Z","iopub.execute_input":"2025-04-05T14:06:59.233255Z","iopub.status.idle":"2025-04-05T14:06:59.253415Z","shell.execute_reply.started":"2025-04-05T14:06:59.233220Z","shell.execute_reply":"2025-04-05T14:06:59.252797Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Memory monitoring function\ndef print_memory_stats():\n    \"\"\"Print current memory usage stats\"\"\"\n    # RAM usage\n    process = psutil.Process(os.getpid())\n    ram_usage = process.memory_info().rss / (1024 * 1024)  # in MB\n    \n    # GPU memory if available\n    gpu_usage = 0\n    if torch.cuda.is_available():\n        gpu_usage = torch.cuda.memory_allocated() / (1024 * 1024)  # in MB\n        max_gpu = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)\n        \n        print(f\"RAM Usage: {ram_usage:.2f} MB\")\n        print(f\"GPU Memory Usage: {gpu_usage:.2f} MB / {max_gpu:.2f} MB ({gpu_usage/max_gpu*100:.1f}%)\")\n    else:\n        print(f\"RAM Usage: {ram_usage:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.254286Z","iopub.execute_input":"2025-04-05T14:06:59.254554Z","iopub.status.idle":"2025-04-05T14:06:59.260476Z","shell.execute_reply.started":"2025-04-05T14:06:59.254526Z","shell.execute_reply":"2025-04-05T14:06:59.259716Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# LRU Cache implementation for memory-efficient caching\nclass LRUCache:\n    \"\"\"\n    LRU (Least Recently Used) Cache for efficient memory usage\n    \"\"\"\n    def __init__(self, capacity):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n    \n    def get(self, key):\n        if key not in self.cache:\n            return None\n        # Move the accessed item to the end (most recently used)\n        self.cache.move_to_end(key)\n        return self.cache[key]\n    \n    def put(self, key, value):\n        if key in self.cache:\n            # Move the updated item to the end\n            self.cache.move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            # Remove the least recently used item\n            self.cache.popitem(last=False)\n        self.cache[key] = value\n    \n    def __len__(self):\n        return len(self.cache)\n        \n    def popitem(self, last=False):\n        \"\"\"Remove and return an item from the cache\"\"\"\n        return self.cache.popitem(last=last)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.261195Z","iopub.execute_input":"2025-04-05T14:06:59.261424Z","iopub.status.idle":"2025-04-05T14:06:59.274318Z","shell.execute_reply.started":"2025-04-05T14:06:59.261394Z","shell.execute_reply":"2025-04-05T14:06:59.273730Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class GraphAttentionLayer(nn.Module):\n    \"\"\"\n    Graph Attention Layer (GAT) implementation with batch processing support\n    \"\"\"\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(GraphAttentionLayer, self).__init__()\n        self.dropout = dropout\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.concat = concat\n\n        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, h, adj):\n        \"\"\"\n        Forward pass with support for batch processing\n        h: Node features of shape (batch_size, N, in_features)\n        adj: Adjacency matrices of shape (batch_size, N, N)\n        \"\"\"\n        batch_size, N, _ = h.size()\n        \n        # Apply feature transformation to all nodes in all graphs - (batch_size, N, out_features)\n        Wh = torch.matmul(h, self.W)\n        \n        # Prepare inputs for attention mechanism\n        # Repeat first dimension for comparisons\n        Wh1 = Wh.unsqueeze(2).repeat(1, 1, N, 1)  # (batch_size, N, N, out_features)\n        # Repeat second dimension for comparisons\n        Wh2 = Wh.unsqueeze(1).repeat(1, N, 1, 1)  # (batch_size, N, N, out_features)\n        \n        # Concatenate for attention calculation\n        a_input = torch.cat([Wh1, Wh2], dim=-1)  # (batch_size, N, N, 2*out_features)\n        \n        # Apply attention mechanism\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))  # (batch_size, N, N)\n        \n        # Mask attention scores using adjacency matrix\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        \n        # Apply softmax and dropout\n        attention = F.softmax(attention, dim=2)  # Normalize on the third dimension (N)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n        \n        # Apply attention to transform features\n        h_prime = torch.matmul(attention, Wh)  # (batch_size, N, out_features)\n        \n        # Apply non-linearity if needed\n        if self.concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.275155Z","iopub.execute_input":"2025-04-05T14:06:59.275390Z","iopub.status.idle":"2025-04-05T14:06:59.286963Z","shell.execute_reply.started":"2025-04-05T14:06:59.275371Z","shell.execute_reply":"2025-04-05T14:06:59.286139Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class GAT(nn.Module):\n    \"\"\"\n    Graph Attention Network with multiple attention heads and batch processing support\n    \"\"\"\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        super(GAT, self).__init__()\n        self.dropout = dropout\n        self.nheads = nheads\n\n        # Multi-head attention layers\n        self.attentions = nn.ModuleList([\n            GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) \n            for _ in range(nheads)\n        ])\n        \n        self.out_att = GraphAttentionLayer(\n            nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False\n        )\n\n    def forward(self, x, adj):\n        \"\"\"\n        Forward pass with batch processing support\n        x: Node features of shape (batch_size, N, in_features)\n        adj: Adjacency matrices of shape (batch_size, N, N)\n        \"\"\"\n        x = F.dropout(x, self.dropout, training=self.training)\n        \n        # Apply each attention head and concatenate results\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=2)\n        \n        x = F.dropout(x, self.dropout, training=self.training)\n        x = F.elu(self.out_att(x, adj))\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.289363Z","iopub.execute_input":"2025-04-05T14:06:59.289582Z","iopub.status.idle":"2025-04-05T14:06:59.301024Z","shell.execute_reply.started":"2025-04-05T14:06:59.289564Z","shell.execute_reply":"2025-04-05T14:06:59.300184Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class SincConv(nn.Module):\n    \"\"\"Sinc-based convolution for raw waveform processing\"\"\"\n    def __init__(self, out_channels, kernel_size, sample_rate=16000, \n                 in_channels=1, stride=1, padding=0, dilation=1, bias=False):\n        super(SincConv, self).__init__()\n        \n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.sample_rate = sample_rate\n        \n        # Initialize filterbanks with mel scale\n        self.freq_low = 50 / (sample_rate/2)  # Normalized low frequency\n        self.freq_high = 7500 / (sample_rate/2)  # Normalized high frequency\n        \n        # Compute the mel scale frequencies\n        self.mel_low = 2595 * np.log10(1 + self.freq_low * (sample_rate/2) / 700)\n        self.mel_high = 2595 * np.log10(1 + self.freq_high * (sample_rate/2) / 700)\n        \n        # Equally spaced in mel scale\n        mel_points = torch.linspace(self.mel_low, self.mel_high, out_channels + 1)\n        \n        # Convert back to frequency domain\n        f_pts = 700 * (10 ** (mel_points / 2595) - 1)\n        \n        # Normalize to [0, 1]\n        self.freq_bands = f_pts / (sample_rate/2)\n        \n        # Ensure proper dimensions for the filters and band_widths\n        self.filters = nn.Parameter(self.freq_bands[:-1].unsqueeze(-1))      # (out_channels, 1)\n        self.band_widths = nn.Parameter((self.freq_bands[1:] - self.freq_bands[:-1]).unsqueeze(-1))  # (out_channels, 1)\n        \n        # Non trainable\n        n = torch.arange(-(kernel_size-1)/2, (kernel_size-1)/2 + 1)\n        self.n = nn.Parameter(n.float(), requires_grad=False)  # (kernel_size,)\n        \n        # Window\n        window = 0.54 - 0.46 * torch.cos(2 * np.pi * torch.arange(kernel_size) / kernel_size)\n        self.window = nn.Parameter(window, requires_grad=False)  # (kernel_size,)\n\n    def forward(self, x):\n        # Get input dimensions\n        batch_size, channels, signal_length = x.shape\n        \n        # Reshape n for proper broadcasting\n        n = self.n.view(1, -1, 1)  # (1, kernel_size, 1)\n        \n        # Get filter center frequencies and bandwidths\n        filters = self.filters.view(-1, 1, 1)  # (out_channels, 1, 1)\n        band_widths = self.band_widths.view(-1, 1, 1)  # (out_channels, 1, 1)\n        \n        # Compute sinc filters\n        low_pass1 = 2 * filters * torch.sinc(2 * filters * n)  # (out_channels, kernel_size, 1)\n        low_pass2 = 2 * (filters + band_widths) * torch.sinc(2 * (filters + band_widths) * n)  # (out_channels, kernel_size, 1)\n        band_pass = low_pass2 - low_pass1  # (out_channels, kernel_size, 1)\n        \n        # Apply window function\n        band_pass = band_pass * self.window.view(1, -1, 1)  # (out_channels, kernel_size, 1)\n        \n        # Normalize\n        band_pass = band_pass / (torch.norm(band_pass, p=2, dim=1, keepdim=True) + 1e-8)  # (out_channels, kernel_size, 1)\n        \n        # Reshape for convolution (out_channels, in_channels, kernel_size)\n        filters = band_pass.squeeze(-1).view(self.out_channels, 1, self.kernel_size)  # (out_channels, 1, kernel_size)\n        \n        # Convolve \n        return F.conv1d(x, filters, stride=1, padding=(self.kernel_size-1)//2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.302430Z","iopub.execute_input":"2025-04-05T14:06:59.302620Z","iopub.status.idle":"2025-04-05T14:06:59.312109Z","shell.execute_reply.started":"2025-04-05T14:06:59.302604Z","shell.execute_reply":"2025-04-05T14:06:59.311406Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    \"\"\"Residual block for feature extraction\"\"\"\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResBlock, self).__init__()\n        \n        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm1d(out_channels)\n        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm1d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm1d(out_channels)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.312990Z","iopub.execute_input":"2025-04-05T14:06:59.313201Z","iopub.status.idle":"2025-04-05T14:06:59.330098Z","shell.execute_reply.started":"2025-04-05T14:06:59.313183Z","shell.execute_reply":"2025-04-05T14:06:59.329432Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class SpectralGraphTransform(nn.Module):\n    \"\"\"Transform raw features into graph features\"\"\"\n    def __init__(self, in_features, hidden_features, out_features):\n        super(SpectralGraphTransform, self).__init__()\n        \n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.330822Z","iopub.execute_input":"2025-04-05T14:06:59.331045Z","iopub.status.idle":"2025-04-05T14:06:59.344599Z","shell.execute_reply.started":"2025-04-05T14:06:59.331027Z","shell.execute_reply":"2025-04-05T14:06:59.343815Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class AASIST(nn.Module):\n    \"\"\"\n    AASIST: Audio Anti-Spoofing using Integrated Spectro-Temporal Graph Attention Networks\n    Optimized for batch processing\n    \"\"\"\n    def __init__(self, \n                 sinc_out_channels=70, \n                 sinc_kernel_size=1024, \n                 sample_rate=16000,\n                 res_channels=[32, 64, 128],\n                 gat_nfeat=128,\n                 gat_nhid=64,\n                 gat_nclass=32,\n                 gat_nheads=8,\n                 gat_alpha=0.2,\n                 gat_dropout=0.3,\n                 n_frame_node=64):\n        super(AASIST, self).__init__()\n        \n        # Raw waveform feature extraction using SincConv\n        self.sinc_conv = SincConv(\n            out_channels=sinc_out_channels,\n            kernel_size=sinc_kernel_size,\n            sample_rate=sample_rate\n        )\n        \n        # Residual blocks for feature processing\n        self.res_block1 = ResBlock(sinc_out_channels, res_channels[0], stride=2)\n        self.res_block2 = ResBlock(res_channels[0], res_channels[1], stride=2)\n        self.res_block3 = ResBlock(res_channels[1], res_channels[2], stride=2)\n        \n        # Feature transformers for graph\n        self.frame_transform = SpectralGraphTransform(\n            res_channels[2], gat_nfeat*2, gat_nfeat\n        )\n        \n        # Number of frame-level nodes for the adjacency matrix\n        self.n_frame_node = n_frame_node\n        \n        # Graph attention network\n        self.gat = GAT(\n            nfeat=gat_nfeat,\n            nhid=gat_nhid,\n            nclass=gat_nclass,\n            dropout=gat_dropout,\n            alpha=gat_alpha,\n            nheads=gat_nheads\n        )\n        \n        # Output layer\n        self.output = nn.Linear(gat_nclass, 1)\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass with efficient batch processing\n        x: Input audio of shape (batch_size, 1, signal_length)\n        \"\"\"\n        batch_size = x.size(0)\n        \n        # Extract features using SincConv\n        x = self.sinc_conv(x)\n        x = F.relu(x)\n        \n        # Process through residual blocks\n        x = self.res_block1(x)\n        x = self.res_block2(x)\n        x = self.res_block3(x)\n        \n        # Adaptive pooling to ensure fixed number of frames\n        x = F.adaptive_avg_pool1d(x, self.n_frame_node)\n        \n        # Prepare for graph processing (batch_size, n_frames, features)\n        x = x.transpose(1, 2)  # (batch_size, n_frame_node, res_channels[2])\n        \n        # Transform features for graph processing\n        frame_feat = self.frame_transform(x)  # (batch_size, n_frame_node, gat_nfeat)\n        \n        # Create adjacency matrices (fully connected) - (batch_size, n_frame_node, n_frame_node)\n        adj = torch.ones(batch_size, self.n_frame_node, self.n_frame_node, device=x.device)\n        \n        # Process all graphs in batch at once using the batch-optimized GAT\n        gat_out = self.gat(frame_feat, adj)  # (batch_size, n_frame_node, gat_nclass)\n        \n        # Global pooling of graph features\n        gat_out = torch.mean(gat_out, dim=1)  # (batch_size, gat_nclass)\n        \n        # Final classification\n        output = torch.sigmoid(self.output(gat_out)).squeeze(-1)  # (batch_size,)\n        \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.345365Z","iopub.execute_input":"2025-04-05T14:06:59.345582Z","iopub.status.idle":"2025-04-05T14:06:59.356486Z","shell.execute_reply.started":"2025-04-05T14:06:59.345565Z","shell.execute_reply":"2025-04-05T14:06:59.355712Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class AASIST_Fixed(AASIST):\n    \"\"\"Modified AASIST model without final sigmoid for use with BCEWithLogitsLoss\"\"\"\n    def forward(self, x):\n        # Keep all the processing the same as the original model\n        batch_size = x.size(0)\n        \n        # Extract features using SincConv\n        x = self.sinc_conv(x)\n        x = F.relu(x)\n        \n        # Process through residual blocks\n        x = self.res_block1(x)\n        x = self.res_block2(x)\n        x = self.res_block3(x)\n        \n        # Adaptive pooling to ensure fixed number of frames\n        x = F.adaptive_avg_pool1d(x, self.n_frame_node)\n        \n        # Prepare for graph processing (batch_size, n_frames, features)\n        x = x.transpose(1, 2)  # (batch_size, n_frame_node, res_channels[2])\n        \n        # Transform features for graph processing\n        frame_feat = self.frame_transform(x)  # (batch_size, n_frame_node, gat_nfeat)\n        \n        # Create adjacency matrices (fully connected) - (batch_size, n_frame_node, n_frame_node)\n        adj = torch.ones(batch_size, self.n_frame_node, self.n_frame_node, device=x.device)\n        \n        # Process all graphs in batch at once using the batch-optimized GAT\n        gat_out = self.gat(frame_feat, adj)  # (batch_size, n_frame_node, gat_nclass)\n        \n        # Global pooling of graph features\n        gat_out = torch.mean(gat_out, dim=1)  # (batch_size, gat_nclass)\n        \n        # Final classification - DIFFERENCE: No sigmoid here\n        output = self.output(gat_out).squeeze(-1)  # (batch_size,)\n        \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.357377Z","iopub.execute_input":"2025-04-05T14:06:59.357670Z","iopub.status.idle":"2025-04-05T14:06:59.373141Z","shell.execute_reply.started":"2025-04-05T14:06:59.357623Z","shell.execute_reply":"2025-04-05T14:06:59.372278Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def count_parameters(model):\n    \"\"\"Count the number of trainable parameters in a model\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.373917Z","iopub.execute_input":"2025-04-05T14:06:59.374160Z","iopub.status.idle":"2025-04-05T14:06:59.386155Z","shell.execute_reply.started":"2025-04-05T14:06:59.374132Z","shell.execute_reply":"2025-04-05T14:06:59.385530Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Test the SincConv layer\ndef test_sincconv():\n    print(\"Testing SincConv layer...\")\n    # Create SincConv instance\n    sinc_conv = SincConv(\n        out_channels=70,\n        kernel_size=1024,\n        sample_rate=16000\n    ).to(device)\n    \n    # Create random input\n    x = torch.randn(2, 1, 16000).to(device)  # (batch_size, channels, signal_length)\n    \n    # Forward pass\n    try:\n        out = sinc_conv(x)\n        print(f\"SincConv test passed! Input shape: {x.shape}, Output shape: {out.shape}\")\n        return True\n    except Exception as e:\n        print(f\"SincConv test failed: {e}\")\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.386907Z","iopub.execute_input":"2025-04-05T14:06:59.387244Z","iopub.status.idle":"2025-04-05T14:06:59.397492Z","shell.execute_reply.started":"2025-04-05T14:06:59.387150Z","shell.execute_reply":"2025-04-05T14:06:59.396721Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class AudioAugmentation:\n    \"\"\"Enhanced audio augmentation techniques with GPU-compatible implementation\"\"\"\n    def __init__(self, sample_rate=16000):\n        self.sample_rate = sample_rate\n    \n    def add_noise(self, audio, noise_level=0.005):\n        \"\"\"Add Gaussian noise to audio\"\"\"\n        # Generate noise on same device as audio\n        noise = torch.randn_like(audio) * noise_level\n        return audio + noise\n    \n    def time_shift(self, audio, shift_range=0.1):\n        \"\"\"Random time shift\"\"\"\n        shift = int(audio.shape[1] * shift_range)\n        if shift > 0:\n            direction = random.choice([1, -1])\n            shift = direction * shift\n            if shift > 0:\n                # Shift right\n                audio = torch.cat([torch.zeros_like(audio[:, :shift]), audio[:, :-shift]], dim=1)\n            else:\n                # Shift left\n                audio = torch.cat([audio[:, -shift:], torch.zeros_like(audio[:, :shift])], dim=1)\n        return audio\n    \n    def speed_change(self, audio, speed_range=0.3):\n        \"\"\"Change playback speed without affecting pitch\"\"\"\n        # Get device of input tensor\n        device = audio.device\n        \n        # Convert to numpy after moving to CPU\n        audio_np = audio.cpu().squeeze().numpy()\n        \n        # Random speed factor\n        speed_factor = random.uniform(1 - speed_range, 1 + speed_range)\n        \n        # Speed up or slow down\n        indices = np.round(np.arange(0, len(audio_np), speed_factor)).astype(int)\n        indices = indices[indices < len(audio_np)]\n        \n        # Get modified audio\n        modified_audio = audio_np[indices]\n        \n        # Convert back to tensor and move to the original device\n        modified_tensor = torch.from_numpy(modified_audio).float().unsqueeze(0).to(device)\n        \n        # Ensure shape matches original\n        if modified_tensor.shape[1] < audio.shape[1]:\n            # Pad\n            padding = audio.shape[1] - modified_tensor.shape[1]\n            modified_tensor = torch.nn.functional.pad(modified_tensor, (0, padding))\n        elif modified_tensor.shape[1] > audio.shape[1]:\n            # Truncate\n            modified_tensor = modified_tensor[:, :audio.shape[1]]\n        \n        return modified_tensor\n    \n    def apply_batch_augmentation(self, audio_batch):\n        \"\"\"Apply augmentation to a batch of audio efficiently\"\"\"\n        batch_size = audio_batch.size(0)\n        # Clone to avoid modifying original tensor\n        augmented_batch = audio_batch.clone()\n        \n        for i in range(batch_size):\n            # Randomly select augmentation type with higher probability (80%)\n            if random.random() < 0.8:\n                aug_type = random.choice(['noise', 'time_shift', 'speed_change'])\n                \n                if aug_type == 'noise':\n                    # Randomize noise level\n                    noise_level = random.uniform(0.001, 0.01)\n                    augmented_batch[i:i+1] = self.add_noise(audio_batch[i:i+1], noise_level)\n                \n                elif aug_type == 'time_shift':\n                    # Randomize shift range\n                    shift_range = random.uniform(0.05, 0.2)\n                    augmented_batch[i:i+1] = self.time_shift(audio_batch[i:i+1], shift_range)\n                \n                elif aug_type == 'speed_change':\n                    # Randomize speed range\n                    speed_range = random.uniform(0.1, 0.3)  # Reduced range to avoid extreme changes\n                    augmented_batch[i:i+1] = self.speed_change(audio_batch[i:i+1], speed_range)\n        \n        return augmented_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.398176Z","iopub.execute_input":"2025-04-05T14:06:59.398373Z","iopub.status.idle":"2025-04-05T14:06:59.409419Z","shell.execute_reply.started":"2025-04-05T14:06:59.398355Z","shell.execute_reply":"2025-04-05T14:06:59.408744Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class ASVspoofDataset(Dataset):\n    \"\"\"\n    Memory-efficient dataset for ASVspoof 2019 Logical Access\n    \"\"\"\n    def __init__(self, \n                 path, \n                 protocol_file, \n                 sample_rate=16000, \n                 max_frames=64000,  # 4 seconds at 16kHz\n                 augment=False,\n                 cache_capacity=200,  # Reduced capacity for lower memory usage\n                 max_cache_size_mb=500):  # Maximum cache size in MB\n        \n        self.path = path\n        self.sample_rate = sample_rate\n        self.max_frames = max_frames\n        self.augment = augment\n        self.max_cache_size_mb = max_cache_size_mb\n        \n        # Print the current working directory for debugging\n        print(f\"Current working directory: {os.getcwd()}\")\n        print(f\"Base path: {path}\")\n        \n        # Determine the protocols directory\n        if os.path.basename(path) == 'LA':\n            # We're at the LA directory level\n            protocols_dir = os.path.join(path, 'ASVspoof2019_LA_cm_protocols')\n        else:\n            # We might be at a higher level\n            protocols_dir = os.path.join(path, 'LA', 'ASVspoof2019_LA_cm_protocols')\n            if not os.path.exists(protocols_dir):\n                # Try another common structure\n                protocols_dir = os.path.join(path, 'ASVspoof2019_LA_cm_protocols')\n        \n        # Get the protocol file path\n        if os.path.isabs(protocol_file):\n            protocol_path = protocol_file\n        else:\n            protocol_path = os.path.join(protocols_dir, os.path.basename(protocol_file))\n            if not os.path.exists(protocol_path):\n                # Try other locations\n                potential_paths = [\n                    os.path.join(path, protocol_file),\n                    os.path.join(path, 'LA', protocol_file),\n                    os.path.join(path, 'LA', 'ASVspoof2019_LA_cm_protocols', protocol_file),\n                    os.path.join(protocols_dir, protocol_file)\n                ]\n                for p in potential_paths:\n                    if os.path.exists(p):\n                        protocol_path = p\n                        break\n        \n        print(f\"Using protocol file: {protocol_path}\")\n        \n        # Determine audio directory based on protocol file\n        protocol_basename = os.path.basename(protocol_path)\n        if 'train' in protocol_basename:\n            if os.path.basename(path) == 'LA':\n                # We're at the LA directory level\n                self.audio_dir = os.path.join(path, 'ASVspoof2019_LA_train', 'flac')\n            else:\n                # Try different possible paths\n                potential_audio_dirs = [\n                    os.path.join(path, 'ASVspoof2019_LA_train', 'flac'),\n                    os.path.join(path, 'LA', 'ASVspoof2019_LA_train', 'flac')\n                ]\n                for d in potential_audio_dirs:\n                    if os.path.exists(d):\n                        self.audio_dir = d\n                        break\n        elif 'dev' in protocol_basename:\n            if os.path.basename(path) == 'LA':\n                self.audio_dir = os.path.join(path, 'ASVspoof2019_LA_dev', 'flac')\n            else:\n                potential_audio_dirs = [\n                    os.path.join(path, 'ASVspoof2019_LA_dev', 'flac'),\n                    os.path.join(path, 'LA', 'ASVspoof2019_LA_dev', 'flac')\n                ]\n                for d in potential_audio_dirs:\n                    if os.path.exists(d):\n                        self.audio_dir = d\n                        break\n        elif 'eval' in protocol_basename:\n            if os.path.basename(path) == 'LA':\n                self.audio_dir = os.path.join(path, 'ASVspoof2019_LA_eval', 'flac')\n            else:\n                potential_audio_dirs = [\n                    os.path.join(path, 'ASVspoof2019_LA_eval', 'flac'),\n                    os.path.join(path, 'LA', 'ASVspoof2019_LA_eval', 'flac')\n                ]\n                for d in potential_audio_dirs:\n                    if os.path.exists(d):\n                        self.audio_dir = d\n                        break\n        else:\n            # Default case - try to find a suitable directory\n            potential_dirs = [\n                os.path.join(path, 'flac'),\n                os.path.join(path, 'LA', 'flac')\n            ]\n            for d in potential_dirs:\n                if os.path.exists(d):\n                    self.audio_dir = d\n                    break\n            if not hasattr(self, 'audio_dir'):\n                self.audio_dir = os.path.join(path, 'flac')  # Default\n        \n        # Check if audio directory exists\n        if not os.path.exists(self.audio_dir):\n            print(f\"Warning: Audio directory {self.audio_dir} not found!\")\n            print(\"Available directories:\")\n            \n            # Try to list directories at the base path and one level up\n            paths_to_list = [path]\n            if 'LA' in path:\n                parent_path = os.path.dirname(path)\n                paths_to_list.append(parent_path)\n            \n            for p in paths_to_list:\n                try:\n                    print(f\"\\nDirectories in {p}:\")\n                    for d in os.listdir(p):\n                        if os.path.isdir(os.path.join(p, d)):\n                            print(f\"  - {d}\")\n                            try:\n                                # List subdirectories one level down\n                                subdirs = os.listdir(os.path.join(p, d))\n                                print(f\"    Subdirectories: {subdirs[:5]}{'...' if len(subdirs) > 5 else ''}\")\n                            except Exception:\n                                pass\n                except Exception as e:\n                    print(f\"Error listing {p}: {e}\")\n            \n            # As a last resort, try to locate flac files using find\n            print(\"\\nSearching for flac files...\")\n            try:\n                import glob\n                flac_files = glob.glob(os.path.join(path, '**', '*.flac'), recursive=True)\n                if flac_files:\n                    print(f\"Found {len(flac_files)} flac files\")\n                    print(f\"Sample file paths: {flac_files[:3]}\")\n                    # Try to determine the correct audio directory\n                    sample_path = os.path.dirname(flac_files[0])\n                    self.audio_dir = sample_path\n                    print(f\"Setting audio directory to: {self.audio_dir}\")\n            except Exception as e:\n                print(f\"Error searching for flac files: {e}\")\n        \n        print(f\"Using audio directory: {self.audio_dir}\")\n        \n        # Load protocol file\n        try:\n            self.protocol_df = pd.read_csv(protocol_path, sep=' ', header=None)\n            \n            # Check number of columns to determine format\n            if self.protocol_df.shape[1] >= 5:  # Has at least 5 columns\n                self.protocol_df.columns = ['speaker_id', 'file_name', 'environment', 'attack_id', 'spoofing_type'] \n            else:  # Likely the evaluation format\n                if self.protocol_df.shape[1] == 3:  # Simplified format\n                    self.protocol_df.columns = ['speaker_id', 'file_name', 'spoofing_type']\n                else:  # Standard format with missing columns\n                    self.protocol_df.columns = ['speaker_id', 'file_name', '-', '-', 'spoofing_type']\n            \n            # Create file list and labels\n            self.file_list = list(self.protocol_df['file_name'])\n            self.labels = [(1 if str(x).lower() == 'bonafide' else 0) for x in self.protocol_df['spoofing_type']]\n            \n            # Print some information about the protocol\n            print(f\"Protocol file has {self.protocol_df.shape[1]} columns\")\n            print(f\"Protocol columns: {self.protocol_df.columns.tolist()}\")\n            print(f\"First few rows of protocol file:\")\n            print(self.protocol_df.head(3))\n            \n        except Exception as e:\n            print(f\"Error loading protocol file: {e}\")\n            # Create empty lists as fallback\n            self.file_list = []\n            self.labels = []\n        \n        # Create augmenter\n        self.augmenter = AudioAugmentation(sample_rate=sample_rate) if augment else None\n        \n        # Setup LRU cache for efficient memory usage\n        self.cache = LRUCache(cache_capacity)\n        self.current_cache_size = 0  # Track cache size in bytes\n        \n        print(f\"Loaded {self.__len__()} files from {protocol_path}\")\n        # Print distribution of classes\n        bonafide_count = self.labels.count(1)\n        spoof_count = self.labels.count(0)\n        print(f\"Bonafide: {bonafide_count} ({bonafide_count/len(self.labels)*100:.2f}% of total)\")\n        print(f\"Spoofed: {spoof_count} ({spoof_count/len(self.labels)*100:.2f}% of total)\")\n        \n        # Print a few file paths to verify\n        if len(self.file_list) > 0:\n            print(\"Sample expected file paths:\")\n            for i in range(min(3, len(self.file_list))):\n                file_path = os.path.join(self.audio_dir, f\"{self.file_list[i]}.flac\")\n                print(f\"  {file_path} (exists: {os.path.exists(file_path)})\")\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def pad_or_truncate(self, audio):\n        \"\"\"Pad or truncate audio to max_frames\"\"\"\n        if audio.shape[1] < self.max_frames:\n            # Pad audio\n            padding = self.max_frames - audio.shape[1]\n            audio = torch.nn.functional.pad(audio, (0, padding))\n        elif audio.shape[1] > self.max_frames:\n            # Truncate audio - randomize starting point for better diversity if augmenting\n            if self.augment:\n                max_start = audio.shape[1] - self.max_frames\n                start = random.randint(0, max_start)\n                audio = audio[:, start:start+self.max_frames]\n            else:\n                audio = audio[:, :self.max_frames]\n        return audio\n    \n    def check_cache_size(self):\n        \"\"\"Check if cache is too large and clear if necessary\"\"\"\n        max_bytes = self.max_cache_size_mb * 1024 * 1024  # Convert MB to bytes\n        if self.current_cache_size > max_bytes:\n            # Clear half the cache to avoid frequent clearing\n            items_to_remove = len(self.cache) // 2\n            for _ in range(items_to_remove):\n                # LRU cache removes least recently used items\n                self.cache.popitem(last=False)\n            \n            # Force garbage collection\n            gc.collect()\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            \n            # Reset cache size counter (approximation)\n            self.current_cache_size = max_bytes // 2\n    \n    def __getitem__(self, idx):\n        file_name = self.file_list[idx]\n        \n        # Check if audio is in cache\n        audio = self.cache.get(file_name)\n        \n        if audio is None:\n            file_path = os.path.join(self.audio_dir, f\"{file_name}.flac\")\n            \n            try:\n                # Check if file exists before trying to load it\n                if not os.path.exists(file_path):\n                    print(f\"Warning: Audio file does not exist: {file_path}\")\n                    # Try different capitalization or path formats\n                    alternate_paths = [\n                        os.path.join(self.audio_dir, f\"{file_name.upper()}.flac\"),\n                        os.path.join(self.audio_dir, f\"{file_name.lower()}.flac\"),\n                        os.path.join(os.path.dirname(self.audio_dir), f\"{file_name}.flac\"),\n                        os.path.join(os.path.dirname(os.path.dirname(self.audio_dir)), f\"{file_name}.flac\")\n                    ]\n                    \n                    for alt_path in alternate_paths:\n                        if os.path.exists(alt_path):\n                            print(f\"Found alternate path: {alt_path}\")\n                            file_path = alt_path\n                            break\n                \n                if not os.path.exists(file_path):\n                    # If file still doesn't exist, raise error to trigger the exception handler\n                    raise FileNotFoundError(f\"Could not find audio file: {file_path}\")\n                \n                # Load audio file\n                audio, sr = torchaudio.load(file_path)\n                \n                # Resample if needed\n                if sr != self.sample_rate:\n                    resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n                    audio = resampler(audio)\n                \n                # Ensure mono audio\n                if audio.shape[0] > 1:\n                    audio = torch.mean(audio, dim=0, keepdim=True)\n                \n                # Normalize audio\n                audio = audio / (torch.max(torch.abs(audio)) + 1e-8)\n                \n                # Add to cache if there's space\n                audio_size_bytes = audio.element_size() * audio.nelement()\n                if self.current_cache_size + audio_size_bytes < self.max_cache_size_mb * 1024 * 1024:\n                    self.cache.put(file_name, audio.clone())\n                    self.current_cache_size += audio_size_bytes\n                    \n                    # Check if cache is too large\n                    self.check_cache_size()\n                \n            except Exception as e:\n                print(f\"Error loading {file_path}: {e}\")\n                # Return a zero tensor and the label\n                audio = torch.zeros(1, self.max_frames)\n        \n        # Pad or truncate to fixed length\n        audio = self.pad_or_truncate(audio)\n        \n        # Apply augmentation if enabled\n        # Note: We don't apply augmentation here for individual samples\n        # Instead, we'll apply batch augmentation in the training loop\n        \n        label = self.labels[idx]\n        \n        return audio, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.410330Z","iopub.execute_input":"2025-04-05T14:06:59.410558Z","iopub.status.idle":"2025-04-05T14:06:59.438637Z","shell.execute_reply.started":"2025-04-05T14:06:59.410541Z","shell.execute_reply":"2025-04-05T14:06:59.437935Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def get_dataloaders(path, batch_size=32, sample_rate=16000, max_frames=64000, \n                   num_workers=4, augment_train=True, pin_memory=True,\n                   persistent_workers=True):\n    \"\"\"\n    Create DataLoaders for ASVspoof 2019 LA with memory-efficient loading\n    \"\"\"\n    # Check and fix path to match the Kaggle dataset structure\n    print(f\"Original path: {path}\")\n    \n    # Check if the path exists\n    if not os.path.exists(path):\n        print(f\"Warning: Path {path} does not exist!\")\n        \n        # Try to find the correct path\n        potential_paths = [\n            '/kaggle/input/asvspoof2019-la',\n            '/kaggle/input/asvspoof2019-la/LA',\n            '/kaggle/input/asvspoof-2019',\n            '/kaggle/input/asvspoof2019',\n        ]\n        \n        for p in potential_paths:\n            if os.path.exists(p):\n                print(f\"Found alternate path: {p}\")\n                path = p\n                break\n    \n    # Print available directories for debugging\n    print(\"Available directories in dataset path:\")\n    try:\n        for d in os.listdir(path):\n            if os.path.isdir(os.path.join(path, d)):\n                print(f\"  - {d}\")\n    except Exception as e:\n        print(f\"Error listing directories: {e}\")\n    \n    # Check for the 'LA' directory\n    la_path = os.path.join(path, 'LA')\n    if os.path.exists(la_path):\n        # If there's an LA subdirectory, use it as the base path\n        print(f\"Found LA directory: {la_path}\")\n        path = la_path\n    \n    # Find protocol files\n    # First look in the standard location\n    protocols_dir = os.path.join(path, 'ASVspoof2019_LA_cm_protocols')\n    \n    # Define protocol file paths\n    train_protocol_file = 'ASVspoof2019.LA.cm.train.trn.txt'\n    dev_protocol_file = 'ASVspoof2019.LA.cm.dev.trl.txt'\n    eval_protocol_file = 'ASVspoof2019.LA.cm.eval.trl.txt'\n    \n    # Get full paths to protocol files\n    if os.path.exists(protocols_dir):\n        train_protocol = os.path.join(protocols_dir, train_protocol_file)\n        dev_protocol = os.path.join(protocols_dir, dev_protocol_file)\n        eval_protocol = os.path.join(protocols_dir, eval_protocol_file)\n    else:\n        # Try directly in the path\n        train_protocol = os.path.join(path, train_protocol_file)\n        dev_protocol = os.path.join(path, dev_protocol_file)\n        eval_protocol = os.path.join(path, eval_protocol_file)\n    \n    # Verify protocol files exist\n    print(f\"Checking train protocol: {train_protocol} (exists: {os.path.exists(train_protocol)})\")\n    print(f\"Checking dev protocol: {dev_protocol} (exists: {os.path.exists(dev_protocol)})\")\n    print(f\"Checking eval protocol: {eval_protocol} (exists: {os.path.exists(eval_protocol)})\")\n    \n    # If protocol files don't exist, search for them\n    if not os.path.exists(train_protocol):\n        print(\"Searching for protocol files...\")\n        import glob\n        protocol_files = glob.glob(os.path.join(path, '**', train_protocol_file), recursive=True)\n        if protocol_files:\n            print(f\"Found train protocol at: {protocol_files[0]}\")\n            train_protocol = protocol_files[0]\n            \n            # Try to find related protocols in the same directory\n            protocols_dir = os.path.dirname(train_protocol)\n            dev_protocol = os.path.join(protocols_dir, dev_protocol_file)\n            eval_protocol = os.path.join(protocols_dir, eval_protocol_file)\n    \n    # Create datasets with error handling\n    try:\n        print(\"\\nCreating training dataset...\")\n        train_dataset = ASVspoofDataset(\n            path=path,\n            protocol_file=train_protocol,\n            sample_rate=sample_rate,\n            max_frames=max_frames,\n            augment=False  # We'll handle augmentation in the training loop for better efficiency\n        )\n    except Exception as e:\n        print(f\"Error creating train dataset: {e}\")\n        # Create an empty dataset as a fallback\n        train_dataset = ASVspoofDataset(\n            path=path,\n            protocol_file=\"\",\n            sample_rate=sample_rate,\n            max_frames=max_frames,\n            augment=False\n        )\n    \n    try:\n        print(\"\\nCreating development dataset...\")\n        dev_dataset = ASVspoofDataset(\n            path=path,\n            protocol_file=dev_protocol,\n            sample_rate=sample_rate,\n            max_frames=max_frames,\n            augment=False\n        )\n    except Exception as e:\n        print(f\"Error creating dev dataset: {e}\")\n        # Use a copy of the training dataset as fallback\n        print(\"Using training dataset as fallback for development dataset\")\n        dev_dataset = train_dataset\n    \n    try:\n        print(\"\\nCreating evaluation dataset...\")\n        eval_dataset = ASVspoofDataset(\n            path=path,\n            protocol_file=eval_protocol,\n            sample_rate=sample_rate,\n            max_frames=max_frames,\n            augment=False\n        )\n    except Exception as e:\n        print(f\"Error creating eval dataset: {e}\")\n        # Use a copy of the dev dataset as fallback\n        print(\"Using development dataset as fallback for evaluation dataset\")\n        eval_dataset = dev_dataset\n    \n    # Create DataLoaders with error handling\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        prefetch_factor=2 if num_workers > 0 else None,\n        persistent_workers=persistent_workers if num_workers > 0 else False\n    )\n    \n    dev_loader = DataLoader(\n        dev_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=persistent_workers if num_workers > 0 else False\n    )\n    \n    eval_loader = DataLoader(\n        eval_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=max(1, num_workers // 2),  # Use fewer workers for eval\n        pin_memory=pin_memory\n    )\n    \n    return train_loader, dev_loader, eval_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.439487Z","iopub.execute_input":"2025-04-05T14:06:59.439801Z","iopub.status.idle":"2025-04-05T14:06:59.454979Z","shell.execute_reply.started":"2025-04-05T14:06:59.439771Z","shell.execute_reply":"2025-04-05T14:06:59.454198Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def mixup_data(x, y, alpha=0.2):\n    \"\"\"\n    Mixup data augmentation that properly handles device placement\n    \"\"\"\n    # Get the device of input data\n    device = x.device\n    \n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n    \n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(device)\n    \n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    \n    return mixed_x, y_a, y_b, lam\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.455790Z","iopub.execute_input":"2025-04-05T14:06:59.456050Z","iopub.status.idle":"2025-04-05T14:06:59.468950Z","shell.execute_reply.started":"2025-04-05T14:06:59.456024Z","shell.execute_reply":"2025-04-05T14:06:59.468217Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def compute_tdcf(bonafide_scores, spoof_scores, cost_model=None):\n    \"\"\"\n    Compute tandem detection cost function (t-DCF)\n    Simplified implementation for ASVspoof 2019\n    \"\"\"\n    if cost_model is None:\n        # Default cost model as per ASVspoof 2019\n        cost_model = {\n            'Pspoof': 0.05,  # Prior probability of spoofing attack\n            'Cmiss_asv': 1,  # Cost of ASV system rejecting target speaker\n            'Cfa_asv': 10,   # Cost of ASV system accepting impostor\n            'Cmiss_cm': 1,   # Cost of CM system rejecting genuine speech\n            'Cfa_cm': 10,    # Cost of CM system accepting spoofed speech\n            'beta_cm': 0.5   # Weight for countermeasure errors\n        }\n    \n    # Sort scores\n    bonafide_scores = sorted(bonafide_scores)\n    spoof_scores = sorted(spoof_scores)\n    \n    # Calculate FRR (genuine) and FAR (spoof) for different thresholds\n    thresholds = sorted(list(set(bonafide_scores + spoof_scores)))\n    \n    frr_list = []\n    far_list = []\n    \n    for threshold in thresholds:\n        frr = sum(score > threshold for score in bonafide_scores) / len(bonafide_scores)\n        far = sum(score <= threshold for score in spoof_scores) / len(spoof_scores)\n        frr_list.append(frr)\n        far_list.append(far)\n    \n    # Calculate t-DCF for each threshold\n    tdcf_list = []\n    for frr, far in zip(frr_list, far_list):\n        # Simplified t-DCF calculation\n        tdcf = (cost_model['Cmiss_cm'] * frr * (1 - cost_model['Pspoof'])) + \\\n               (cost_model['Cfa_cm'] * far * cost_model['Pspoof'])\n        tdcf_list.append(tdcf)\n    \n    # Find minimum t-DCF\n    min_tdcf = min(tdcf_list)\n    min_tdcf_threshold = thresholds[tdcf_list.index(min_tdcf)]\n    \n    return min_tdcf, min_tdcf_threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.469578Z","iopub.execute_input":"2025-04-05T14:06:59.469805Z","iopub.status.idle":"2025-04-05T14:06:59.482488Z","shell.execute_reply.started":"2025-04-05T14:06:59.469787Z","shell.execute_reply":"2025-04-05T14:06:59.481717Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Configuration for faster training and lower memory usage\nconfig = {\n    # Training parameters\n    'batch_size': 32,      # Smaller batch size to prevent OOM errors\n    'epochs': 20,          # Reduced epochs \n    'lr': 0.0003,          # Learning rate\n    'weight_decay': 0.0001,\n    'mixup_alpha': 0.2,    # Mixup augmentation parameter\n    'amp': True,           # Enable Automatic Mixed Precision for faster training\n    \n    # Model parameters\n    'model': {\n        'sinc_out_channels': 70,\n        'sinc_kernel_size': 1024,\n        'sample_rate': 16000,\n        'res_channels': [32, 64, 128],\n        'gat_nfeat': 128,\n        'gat_nhid': 64,\n        'gat_nclass': 32,\n        'gat_nheads': 8,\n        'gat_alpha': 0.2,\n        'gat_dropout': 0.3,  # Reduced dropout for faster convergence\n        'n_frame_node': 64\n    },\n    \n    # Data parameters\n    'data': {\n        'sample_rate': 16000,\n        'max_frames': 64000,  # 4 seconds at 16kHz\n        'num_workers': 2,     # Reduced for stability\n        'augment_train': True,\n        'cache_capacity': 100,  # Number of files to cache\n        'max_cache_size_mb': 300  # Maximum cache size in MB\n    },\n    \n    # Optimization parameters\n    'optimizer': 'AdamW',  # Options: 'Adam', 'AdamW', 'SGD'\n    'scheduler': 'OneCycleLR',  # Options: 'CosineAnnealingWarmRestarts', 'OneCycleLR', 'ReduceLROnPlateau'\n    \n    # Paths - will be updated automatically\n    'data_path': '/kaggle/input/asvspoof2019-la/LA',\n    'output_dir': './outputs'\n}\n\n# Create output directory\nos.makedirs(config['output_dir'], exist_ok=True)\n\n# Detect and configure ASVspoof dataset path\nprint(\"Detecting ASVspoof dataset location on Kaggle...\")\n\n# List all available datasets in the input directory\nprint(\"Available datasets in Kaggle input directory:\")\n!ls -la /kaggle/input/\n\n# Check for ASVspoof in the directory names\nasvspoof_dirs = []\nfor root, dirs, files in os.walk('/kaggle/input'):\n    for d in dirs:\n        if 'asvspoof' in d.lower():\n            asvspoof_dirs.append(os.path.join(root, d))\n\nif asvspoof_dirs:\n    print(f\"Found potential ASVspoof directories: {asvspoof_dirs}\")\n    \n    # Find the most likely candidate (with LA or logical_access in name)\n    best_dir = None\n    for d in asvspoof_dirs:\n        # Check if this directory has LA structure\n        if os.path.exists(os.path.join(d, 'LA')):\n            best_dir = os.path.join(d, 'LA')\n            break\n        elif 'LA' in d or 'logical' in d.lower() or 'logical_access' in d.lower():\n            best_dir = d\n            break\n    \n    # If no obvious best directory, just use the first one\n    if best_dir is None and asvspoof_dirs:\n        best_dir = asvspoof_dirs[0]\n    \n    if best_dir:\n        print(f\"Setting dataset path to: {best_dir}\")\n        config['data_path'] = best_dir\n        \n        # Check for specific paths known to work\n        if 'asvspoof2019-la' in best_dir:\n            print(\"Detected asvspoof2019-la dataset, using known structure\")\n            \n            # If path is to asvspoof2019-la, add LA subdirectory \n            if not best_dir.endswith('/LA'):\n                config['data_path'] = os.path.join(best_dir, 'LA')\n                print(f\"Updated path to include LA directory: {config['data_path']}\")\nelse:\n    print(\"No ASVspoof directories found. Using default path.\")\n\n# Update configuration based on the structure\nprint(f\"\\nFinal dataset path: {config['data_path']}\")\n\n# Update model and loss function for mixed precision compatibility\nprint(\"Updating model and loss function for mixed precision compatibility...\")\nconfig['use_fixed_model'] = True  # Use the fixed model version\nconfig['use_bce_with_logits'] = True  # Use BCEWithLogitsLoss instead of BCELoss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:06:59.483162Z","iopub.execute_input":"2025-04-05T14:06:59.483365Z","iopub.status.idle":"2025-04-05T14:11:22.302813Z","shell.execute_reply.started":"2025-04-05T14:06:59.483347Z","shell.execute_reply":"2025-04-05T14:11:22.301847Z"}},"outputs":[{"name":"stdout","text":"Detecting ASVspoof dataset location on Kaggle...\nAvailable datasets in Kaggle input directory:\ntotal 8\ndrwxr-xr-x 3 root   root    4096 Apr  5 14:06 .\ndrwxr-xr-x 5 root   root    4096 Apr  5 14:06 ..\ndrwxr-xr-x 3 nobody nogroup    0 Apr  3 14:46 asvspoof2019-la\nFound potential ASVspoof directories: ['/kaggle/input/asvspoof2019-la', '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_dev', '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_train', '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols', '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_eval', '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_asv_protocols', '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_asv_scores']\nSetting dataset path to: /kaggle/input/asvspoof2019-la/LA\nDetected asvspoof2019-la dataset, using known structure\n\nFinal dataset path: /kaggle/input/asvspoof2019-la/LA\nUpdating model and loss function for mixed precision compatibility...\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Update AMP functions to use the newer syntax\ndef get_amp_autocast():\n    \"\"\"Get the appropriate autocast context manager\"\"\"\n    if torch.cuda.is_available():\n        return torch.amp.autocast(device_type='cuda')\n    else:\n        return torch.amp.autocast(device_type='cpu')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:11:22.304093Z","iopub.execute_input":"2025-04-05T14:11:22.304435Z","iopub.status.idle":"2025-04-05T14:11:22.308624Z","shell.execute_reply.started":"2025-04-05T14:11:22.304400Z","shell.execute_reply":"2025-04-05T14:11:22.307823Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def get_amp_scaler():\n    \"\"\"Get the appropriate GradScaler\"\"\"\n    if torch.cuda.is_available():\n        return torch.amp.GradScaler()\n    else:\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:11:22.312211Z","iopub.execute_input":"2025-04-05T14:11:22.312427Z","iopub.status.idle":"2025-04-05T14:11:22.321402Z","shell.execute_reply.started":"2025-04-05T14:11:22.312409Z","shell.execute_reply":"2025-04-05T14:11:22.320562Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def main():\n    # Set random seed\n    setup_seed(42)\n    \n    # Print initial memory stats\n    print(\"Initial memory usage:\")\n    print_memory_stats()\n    \n    # Test SincConv before proceeding\n    print(\"\\nTesting SincConv implementation:\")\n    test_sincconv()\n    \n    # Create data loaders\n    try:\n        train_loader, dev_loader, eval_loader = get_dataloaders(\n            path=config['data_path'],\n            batch_size=config['batch_size'],\n            sample_rate=config['data']['sample_rate'],\n            max_frames=config['data']['max_frames'],\n            num_workers=config['data']['num_workers'],\n            augment_train=config['data']['augment_train']\n        )\n    except Exception as e:\n        print(f\"Error creating dataloaders: {e}\")\n        # Create empty dataloaders\n        train_loader = []\n        dev_loader = []\n        eval_loader = []\n    \n    # Create model based on configuration\n    if config.get('use_fixed_model', False):\n        print(\"Using fixed model without sigmoid for BCEWithLogitsLoss\")\n        model = AASIST_Fixed(\n            sinc_out_channels=config['model']['sinc_out_channels'],\n            sinc_kernel_size=config['model']['sinc_kernel_size'],\n            sample_rate=config['model']['sample_rate'],\n            res_channels=config['model']['res_channels'],\n            gat_nfeat=config['model']['gat_nfeat'],\n            gat_nhid=config['model']['gat_nhid'],\n            gat_nclass=config['model']['gat_nclass'],\n            gat_nheads=config['model']['gat_nheads'],\n            gat_alpha=config['model']['gat_alpha'],\n            gat_dropout=config['model']['gat_dropout'],\n            n_frame_node=config['model']['n_frame_node']\n        ).to(device)\n    else:\n        print(\"Using original model with sigmoid and BCELoss\")\n        model = AASIST(\n            sinc_out_channels=config['model']['sinc_out_channels'],\n            sinc_kernel_size=config['model']['sinc_kernel_size'],\n            sample_rate=config['model']['sample_rate'],\n            res_channels=config['model']['res_channels'],\n            gat_nfeat=config['model']['gat_nfeat'],\n            gat_nhid=config['model']['gat_nhid'],\n            gat_nclass=config['model']['gat_nclass'],\n            gat_nheads=config['model']['gat_nheads'],\n            gat_alpha=config['model']['gat_alpha'],\n            gat_dropout=config['model']['gat_dropout'],\n            n_frame_node=config['model']['n_frame_node']\n        ).to(device)\n    \n    print(f\"Model has {count_parameters(model):,} trainable parameters\")\n    \n    # Create optimizer\n    if config['optimizer'] == 'Adam':\n        optimizer = optim.Adam(\n            model.parameters(), \n            lr=config['lr'], \n            weight_decay=config['weight_decay']\n        )\n    elif config['optimizer'] == 'AdamW':\n        optimizer = optim.AdamW(\n            model.parameters(), \n            lr=config['lr'], \n            weight_decay=config['weight_decay']\n        )\n    elif config['optimizer'] == 'SGD':\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=config['lr'],\n            momentum=0.9,\n            weight_decay=config['weight_decay'],\n            nesterov=True\n        )\n    \n    # Check if data loaders are empty\n    if not train_loader or len(train_loader) == 0:\n        print(\"Error: Empty train loader. Cannot continue training.\")\n        return None, {}\n    \n    # Create learning rate scheduler\n    if config['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer,\n            T_0=10,  # Restart every 10 epochs\n            T_mult=1,\n            eta_min=config['lr'] / 10\n        )\n    elif config['scheduler'] == 'OneCycleLR':\n        scheduler = optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=config['lr'],\n            epochs=config['epochs'],\n            steps_per_epoch=len(train_loader),\n            pct_start=0.2,  # Warm up for 20% of training\n            div_factor=25,  # Initial lr = max_lr/25\n            final_div_factor=1000  # Final lr = max_lr/1000\n        )\n    elif config['scheduler'] == 'ReduceLROnPlateau':\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \n            mode='min', \n            factor=0.5, \n            patience=3, \n            verbose=True\n        )\n    \n    # Create loss function\n    if config.get('use_bce_with_logits', False):\n        print(\"Using BCEWithLogitsLoss for mixed precision compatibility\")\n        criterion = nn.BCEWithLogitsLoss()\n    else:\n        print(\"Using standard BCELoss (not compatible with mixed precision)\")\n        criterion = nn.BCELoss()\n    \n    # Create tensorboard writer\n    writer = SummaryWriter(os.path.join(config['output_dir'], 'logs'))\n    \n    # Create augmenter\n    augmenter = AudioAugmentation(sample_rate=config['data']['sample_rate'])\n    \n    # Initialize tracking variables\n    best_eer = float('inf')\n    best_epoch = 0\n    \n    # Print start message\n    print(f\"Starting training for {config['epochs']} epochs\")\n    \n    # Initialize Automatic Mixed Precision (AMP) components\n    use_amp = config['amp']\n    scaler = get_amp_scaler() if use_amp else None\n    \n    # Training loop\n    for epoch in range(config['epochs']):\n        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n        \n        # Train\n        model.train()\n        running_loss = 0.0\n        pred_scores = []\n        true_labels = []\n        \n        # Training loop with fixed autocast\n        pbar = tqdm(train_loader, desc=\"Training\")\n        for batch_idx, (audio, labels) in enumerate(pbar):\n            try:\n                # Move data to device\n                audio = audio.to(device)\n                labels = labels.float().to(device)\n                \n                # Apply batch augmentation if enabled\n                if augmenter is not None:\n                    try:\n                        audio = augmenter.apply_batch_augmentation(audio)\n                    except Exception as e:\n                        print(f\"Warning: Augmentation failed: {e}. Continuing with original audio.\")\n                \n                # Apply mixup\n                mixup_applied = False\n                if config.get('mixup_alpha', 0) > 0:\n                    try:\n                        audio, labels_a, labels_b, lam = mixup_data(audio, labels, config['mixup_alpha'])\n                        mixup_applied = True\n                    except Exception as e:\n                        print(f\"Warning: Mixup failed: {e}. Continuing without mixup.\")\n                \n                # Forward pass with fixed mixed precision\n                optimizer.zero_grad()\n                \n                if use_amp and scaler is not None:\n                    with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n                        outputs = model(audio)\n                        \n                        if mixup_applied:\n                            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n                        else:\n                            loss = criterion(outputs, labels)\n                    \n                    # Backward and optimize with gradient scaling\n                    scaler.scale(loss).backward()\n                    \n                    # Clip gradients to prevent exploding gradients\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n                    \n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    # Standard forward pass without mixed precision\n                    outputs = model(audio)\n                    \n                    if mixup_applied:\n                        loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n                    else:\n                        loss = criterion(outputs, labels)\n                    \n                    # Backward and optimize\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n                    optimizer.step()\n                \n                # Step scheduler if batch-based\n                if scheduler is not None and isinstance(scheduler, (optim.lr_scheduler.CyclicLR, \n                                                                  optim.lr_scheduler.OneCycleLR)):\n                    scheduler.step()\n                \n                # For metrics, use original labels if not using mixup\n                if not mixup_applied:\n                    # Get scores appropriate to the model type\n                    if isinstance(model, AASIST_Fixed):\n                        scores = torch.sigmoid(outputs).detach().cpu().numpy()\n                    else:\n                        scores = outputs.detach().cpu().numpy()\n                        \n                    pred_scores.extend(scores)\n                    true_labels.extend(labels.detach().cpu().numpy())\n                \n                # Update progress bar\n                running_loss += loss.item()\n                avg_loss = running_loss / (batch_idx + 1)\n                pbar.set_postfix({'loss': avg_loss})\n                \n                # Free up memory\n                del audio, labels, outputs, loss\n                if mixup_applied:\n                    del labels_a, labels_b\n                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n                \n            except Exception as e:\n                print(f\"Error processing batch {batch_idx}: {e}\")\n                # Try to free memory\n                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n                continue\n        \n        # Calculate metrics\n        epoch_loss = running_loss / len(train_loader) if len(train_loader) > 0 else float('inf')\n        \n        # Skip metrics calculation if no predictions were collected\n        if len(pred_scores) > 0:\n            pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n            accuracy = accuracy_score(true_labels, pred_labels)\n            auc = roc_auc_score(true_labels, pred_scores) if len(np.unique(true_labels)) > 1 else 0.5\n        else:\n            accuracy = 0\n            auc = 0\n            \n        # Print training metrics\n        print(f\"Train Loss: {epoch_loss:.4f}, Acc: {accuracy:.4f}, AUC: {auc:.4f}\")\n        \n        # Validate\n        model.eval()\n        val_running_loss = 0.0\n        val_pred_scores = []\n        val_true_labels = []\n        \n        with torch.no_grad():\n            for audio, labels in tqdm(dev_loader, desc=\"Validation\"):\n                try:\n                    # Move data to device\n                    audio = audio.to(device)\n                    labels = labels.float().to(device)\n                    \n                    # Forward pass with fixed mixed precision\n                    if use_amp and torch.cuda.is_available():\n                        with torch.amp.autocast(device_type='cuda'):\n                            outputs = model(audio)\n                            loss = criterion(outputs, labels)\n                    else:\n                        outputs = model(audio)\n                        loss = criterion(outputs, labels)\n                    \n                    # Collect statistics\n                    val_running_loss += loss.item()\n                    \n                    # Get scores appropriate to the model type\n                    if isinstance(model, AASIST_Fixed):\n                        scores = torch.sigmoid(outputs).detach().cpu().numpy()\n                    else:\n                        scores = outputs.detach().cpu().numpy()\n                        \n                    val_pred_scores.extend(scores)\n                    val_true_labels.extend(labels.detach().cpu().numpy())\n                    \n                except Exception as e:\n                    print(f\"Error during validation: {e}\")\n                    continue\n        \n        # Calculate validation metrics\n        val_loss = val_running_loss / len(dev_loader) if len(dev_loader) > 0 else float('inf')\n        \n        if len(val_pred_scores) > 0:\n            val_pred_labels = [1 if score >= 0.5 else 0 for score in val_pred_scores]\n            val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n            val_auc = roc_auc_score(val_true_labels, val_pred_scores) if len(np.unique(val_true_labels)) > 1 else 0.5\n            \n            # Calculate EER\n            fpr, tpr, thresholds = roc_curve(val_true_labels, val_pred_scores)\n            fnr = 1 - tpr\n            eer_idx = np.nanargmin(np.absolute(fnr - fpr))\n            val_eer = fpr[eer_idx]\n            eer_threshold = thresholds[eer_idx]\n        else:\n            val_accuracy = 0\n            val_auc = 0.5\n            val_eer = 0.5\n            eer_threshold = 0.5\n        \n        # Update learning rate for epoch-based schedulers\n        if config['scheduler'] == 'CosineAnnealingWarmRestarts':\n            scheduler.step()\n        elif config['scheduler'] == 'ReduceLROnPlateau':\n            scheduler.step(val_loss)\n        \n        # Save logs\n        writer.add_scalar('Loss/train', epoch_loss, epoch)\n        writer.add_scalar('Loss/val', val_loss, epoch)\n        writer.add_scalar('Accuracy/train', accuracy, epoch)\n        writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n        writer.add_scalar('AUC/train', auc, epoch)\n        writer.add_scalar('AUC/val', val_auc, epoch)\n        writer.add_scalar('EER/val', val_eer, epoch)\n        writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)\n        \n        # Print validation metrics\n        print(f\"Val Loss: {val_loss:.4f}, Acc: {val_accuracy:.4f}, AUC: {val_auc:.4f}, EER: {val_eer:.4f}\")\n        \n        # Memory check after each epoch\n        print(f\"Memory usage after epoch {epoch+1}:\")\n        print_memory_stats()\n        \n        # Save checkpoint (only save every few epochs to save disk space)\n        if (epoch + 1) % 5 == 0 or epoch == config['epochs'] - 1:\n            checkpoint_path = os.path.join(config['output_dir'], f'checkpoint_epoch_{epoch+1}.pth')\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n                'val_eer': val_eer,\n                'best_eer': best_eer,\n                'is_fixed_model': isinstance(model, AASIST_Fixed)\n            }, checkpoint_path)\n        \n        # Save best model\n        if val_eer < best_eer:\n            best_eer = val_eer\n            best_epoch = epoch\n            best_model_path = os.path.join(config['output_dir'], 'best_model.pth')\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n                'val_eer': val_eer,\n                'best_eer': best_eer,\n                'is_fixed_model': isinstance(model, AASIST_Fixed)\n            }, best_model_path)\n            print(f\"New best model saved with EER: {val_eer:.4f}\")\n        \n        # Clean up to prevent memory leaks\n        gc.collect()\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    # Final evaluation on evaluation set\n    print(\"\\nTraining completed!\")\n    print(f\"Best validation EER: {best_eer:.4f} at epoch {best_epoch+1}\")\n    print(\"\\nEvaluating on evaluation set...\")\n    \n    # Load best model\n    best_model_path = os.path.join(config['output_dir'], 'best_model.pth')\n    if os.path.exists(best_model_path):\n        checkpoint = torch.load(best_model_path)\n        \n        # Check if it's a fixed model\n        is_fixed_model = checkpoint.get('is_fixed_model', False)\n        if is_fixed_model and not isinstance(model, AASIST_Fixed):\n            print(\"Loading best model (fixed version without sigmoid)\")\n            model = AASIST_Fixed(\n                sinc_out_channels=config['model']['sinc_out_channels'],\n                sinc_kernel_size=config['model']['sinc_kernel_size'],\n                sample_rate=config['model']['sample_rate'],\n                res_channels=config['model']['res_channels'],\n                gat_nfeat=config['model']['gat_nfeat'],\n                gat_nhid=config['model']['gat_nhid'],\n                gat_nclass=config['model']['gat_nclass'],\n                gat_nheads=config['model']['gat_nheads'],\n                gat_alpha=config['model']['gat_alpha'],\n                gat_dropout=config['model']['gat_dropout'],\n                n_frame_node=config['model']['n_frame_node']\n            ).to(device)\n        elif not is_fixed_model and isinstance(model, AASIST_Fixed):\n            print(\"Loading best model (original version with sigmoid)\")\n            model = AASIST(\n                sinc_out_channels=config['model']['sinc_out_channels'],\n                sinc_kernel_size=config['model']['sinc_kernel_size'],\n                sample_rate=config['model']['sample_rate'],\n                res_channels=config['model']['res_channels'],\n                gat_nfeat=config['model']['gat_nfeat'],\n                gat_nhid=config['model']['gat_nhid'],\n                gat_nclass=config['model']['gat_nclass'],\n                gat_nheads=config['model']['gat_nheads'],\n                gat_alpha=config['model']['gat_alpha'],\n                gat_dropout=config['model']['gat_dropout'],\n                n_frame_node=config['model']['n_frame_node']\n            ).to(device)\n            \n        model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Evaluate final model\n    model.eval()\n    test_pred_scores = []\n    test_true_labels = []\n    \n    with torch.no_grad():\n        for audio, labels in tqdm(eval_loader, desc=\"Evaluation\"):\n            try:\n                # Move data to device\n                audio = audio.to(device)\n                \n                # Forward pass\n                if use_amp and torch.cuda.is_available():\n                    with torch.amp.autocast(device_type='cuda'):\n                        outputs = model(audio)\n                else:\n                    outputs = model(audio)\n                \n                # Get scores appropriate to the model type\n                if isinstance(model, AASIST_Fixed):\n                    scores = torch.sigmoid(outputs).detach().cpu().numpy()\n                else:\n                    scores = outputs.detach().cpu().numpy()\n                    \n                test_pred_scores.extend(scores)\n                test_true_labels.extend(labels.numpy())\n                \n            except Exception as e:\n                print(f\"Error during evaluation: {e}\")\n                continue\n    \n    # Calculate final evaluation metrics\n    if len(test_pred_scores) > 0:\n        test_pred_labels = [1 if score >= 0.5 else 0 for score in test_pred_scores]\n        test_accuracy = accuracy_score(test_true_labels, test_pred_labels)\n        test_auc = roc_auc_score(test_true_labels, test_pred_scores) if len(np.unique(test_true_labels)) > 1 else 0.5\n        \n        # Calculate EER\n        fpr, tpr, thresholds = roc_curve(test_true_labels, test_pred_scores)\n        fnr = 1 - tpr\n        eer_idx = np.nanargmin(np.absolute(fnr - fpr))\n        test_eer = fpr[eer_idx]\n        test_eer_threshold = thresholds[eer_idx]\n        \n        # Calculate t-DCF\n        bonafide_scores = [test_pred_scores[i] for i in range(len(test_pred_scores)) if test_true_labels[i] == 1]\n        spoof_scores = [test_pred_scores[i] for i in range(len(test_pred_scores)) if test_true_labels[i] == 0]\n        \n        if bonafide_scores and spoof_scores:\n            test_min_tdcf, test_tdcf_threshold = compute_tdcf(bonafide_scores, spoof_scores)\n        else:\n            test_min_tdcf, test_tdcf_threshold = 0.5, 0.5\n    else:\n        test_accuracy = 0\n        test_auc = 0.5\n        test_eer = 0.5\n        test_eer_threshold = 0.5\n        test_min_tdcf = 0.5\n        test_tdcf_threshold = 0.5\n    \n    # Report results\n    print(\"\\nEvaluation Results:\")\n    print(f\"Accuracy: {test_accuracy:.4f}\")\n    print(f\"AUC: {test_auc:.4f}\")\n    print(f\"EER: {test_eer:.4f}\")\n    print(f\"min-tDCF: {test_min_tdcf:.4f}\")\n    print(f\"EER Threshold: {test_eer_threshold:.4f}\")\n    \n    # Save final results\n    results = {\n        'test_accuracy': test_accuracy,\n        'test_auc': test_auc,\n        'test_eer': test_eer,\n        'test_tdcf': test_min_tdcf,\n        'eer_threshold': test_eer_threshold,\n        'tdcf_threshold': test_tdcf_threshold,\n        'best_epoch': best_epoch + 1,\n        'is_fixed_model': isinstance(model, AASIST_Fixed)\n    }\n    \n    with open(os.path.join(config['output_dir'], 'results.yaml'), 'w') as f:\n        yaml.dump(results, f)\n    \n    # Print final memory usage\n    print(\"\\nFinal memory usage:\")\n    print_memory_stats()\n    \n    return model, results\n\n# Run the training function\nif __name__ == \"__main__\":\n    model, results = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:11:22.322671Z","iopub.execute_input":"2025-04-05T14:11:22.322956Z","iopub.status.idle":"2025-04-05T16:55:00.781058Z","shell.execute_reply.started":"2025-04-05T14:11:22.322928Z","shell.execute_reply":"2025-04-05T16:55:00.779836Z"}},"outputs":[{"name":"stdout","text":"Initial memory usage:\nRAM Usage: 739.97 MB\nGPU Memory Usage: 0.00 MB / 16269.25 MB (0.0%)\n\nTesting SincConv implementation:\nTesting SincConv layer...\nSincConv test passed! Input shape: torch.Size([2, 1, 16000]), Output shape: torch.Size([2, 70, 15999])\nOriginal path: /kaggle/input/asvspoof2019-la/LA\nAvailable directories in dataset path:\n  - ASVspoof2019_LA_dev\n  - ASVspoof2019_LA_train\n  - ASVspoof2019_LA_cm_protocols\n  - ASVspoof2019_LA_eval\n  - ASVspoof2019_LA_asv_protocols\n  - ASVspoof2019_LA_asv_scores\nChecking train protocol: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt (exists: True)\nChecking dev protocol: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt (exists: True)\nChecking eval protocol: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt (exists: True)\n\nCreating training dataset...\nCurrent working directory: /kaggle/working\nBase path: /kaggle/input/asvspoof2019-la/LA\nUsing protocol file: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\nUsing audio directory: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_train/flac\nProtocol file has 5 columns\nProtocol columns: ['speaker_id', 'file_name', 'environment', 'attack_id', 'spoofing_type']\nFirst few rows of protocol file:\n  speaker_id     file_name environment attack_id spoofing_type\n0    LA_0079  LA_T_1138215           -         -      bonafide\n1    LA_0079  LA_T_1271820           -         -      bonafide\n2    LA_0079  LA_T_1272637           -         -      bonafide\nLoaded 25380 files from /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\nBonafide: 2580 (10.17% of total)\nSpoofed: 22800 (89.83% of total)\nSample expected file paths:\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_train/flac/LA_T_1138215.flac (exists: True)\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_train/flac/LA_T_1271820.flac (exists: True)\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_train/flac/LA_T_1272637.flac (exists: True)\n\nCreating development dataset...\nCurrent working directory: /kaggle/working\nBase path: /kaggle/input/asvspoof2019-la/LA\nUsing protocol file: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\nUsing audio directory: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_dev/flac\nProtocol file has 5 columns\nProtocol columns: ['speaker_id', 'file_name', 'environment', 'attack_id', 'spoofing_type']\nFirst few rows of protocol file:\n  speaker_id     file_name environment attack_id spoofing_type\n0    LA_0069  LA_D_1047731           -         -      bonafide\n1    LA_0069  LA_D_1105538           -         -      bonafide\n2    LA_0069  LA_D_1125976           -         -      bonafide\nLoaded 24844 files from /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\nBonafide: 2548 (10.26% of total)\nSpoofed: 22296 (89.74% of total)\nSample expected file paths:\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_dev/flac/LA_D_1047731.flac (exists: True)\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_dev/flac/LA_D_1105538.flac (exists: True)\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_dev/flac/LA_D_1125976.flac (exists: True)\n\nCreating evaluation dataset...\nCurrent working directory: /kaggle/working\nBase path: /kaggle/input/asvspoof2019-la/LA\nUsing protocol file: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\nUsing audio directory: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_eval/flac\nProtocol file has 5 columns\nProtocol columns: ['speaker_id', 'file_name', 'environment', 'attack_id', 'spoofing_type']\nFirst few rows of protocol file:\n  speaker_id     file_name environment attack_id spoofing_type\n0    LA_0039  LA_E_2834763           -       A11         spoof\n1    LA_0014  LA_E_8877452           -       A14         spoof\n2    LA_0040  LA_E_6828287           -       A16         spoof\nLoaded 71237 files from /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\nBonafide: 7355 (10.32% of total)\nSpoofed: 63882 (89.68% of total)\nSample expected file paths:\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_eval/flac/LA_E_2834763.flac (exists: True)\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_eval/flac/LA_E_8877452.flac (exists: True)\n  /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_eval/flac/LA_E_6828287.flac (exists: True)\nUsing fixed model without sigmoid for BCEWithLogitsLoss\nModel has 264,877 trainable parameters\nUsing BCEWithLogitsLoss for mixed precision compatibility\nStarting training for 20 epochs\n\nEpoch 1/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a5aefda23af4025b836bd91fda2864e"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.3662, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c0d6a32cbd74670ab4968b0f3ba46ee"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3278, Acc: 0.8974, AUC: 0.5947, EER: 0.4368\nMemory usage after epoch 1:\nRAM Usage: 1689.70 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.4368\n\nEpoch 2/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75814eb677d34e3ab50bf7ea3b62bbdc"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.3216, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45cc3c4d17c47e4983a1bdbcc3ed9b8"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.2917, Acc: 0.8974, AUC: 0.7639, EER: 0.3115\nMemory usage after epoch 2:\nRAM Usage: 1691.84 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.3115\n\nEpoch 3/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74da4386116a42ce8889d30ed7c0a5f0"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.3023, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6709e93d174ef19dbbc475eb606179"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3993, Acc: 0.8987, AUC: 0.8227, EER: 0.2497\nMemory usage after epoch 3:\nRAM Usage: 1692.34 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.2497\n\nEpoch 4/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db81f02d627a474b978e95f13deb4ad0"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.2561, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690e6f83c97c432281ca2f9165955cea"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.1409, Acc: 0.9457, AUC: 0.9615, EER: 0.1067\nMemory usage after epoch 4:\nRAM Usage: 1692.34 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.1067\n\nEpoch 5/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5a9f402ee684c2ebece90cde50a867c"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1937, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ca6b6f99c3d44629ea544ed71a08270"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0799, Acc: 0.9733, AUC: 0.9920, EER: 0.0501\nMemory usage after epoch 5:\nRAM Usage: 1692.34 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0501\n\nEpoch 6/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7698cd7e6f9499cb91696470c497659"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1802, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c6d0c933cac4e6fa51219978ab21be1"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0837, Acc: 0.9702, AUC: 0.9944, EER: 0.0330\nMemory usage after epoch 6:\nRAM Usage: 1692.41 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0330\n\nEpoch 7/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52ea597fe90424da6cc0ab2f6690dce"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1684, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb9be0db78e546d299408ff510cbff3a"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0233, Acc: 0.9928, AUC: 0.9996, EER: 0.0099\nMemory usage after epoch 7:\nRAM Usage: 1692.71 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0099\n\nEpoch 8/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d12ac22097ba4e5dbcc07383f638cfae"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1522, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a09d42e026924facaecfba5a54df13b2"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0174, Acc: 0.9958, AUC: 0.9998, EER: 0.0087\nMemory usage after epoch 8:\nRAM Usage: 1692.71 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0087\n\nEpoch 9/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68bb8a150ce84f01b2f89a63e688fe7f"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1549, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"832d7705d08747a2ab6f9d9fc7a1a472"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0257, Acc: 0.9933, AUC: 0.9998, EER: 0.0067\nMemory usage after epoch 9:\nRAM Usage: 1692.84 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0067\n\nEpoch 10/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e5a2faa7ec343e7be48576d7cbb1feb"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1537, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a69bd8f5ec034072accbf7220f5d9ffb"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0132, Acc: 0.9965, AUC: 0.9999, EER: 0.0047\nMemory usage after epoch 10:\nRAM Usage: 1692.96 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0047\n\nEpoch 11/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f014cd0cbe7147959806f8ef96ff3091"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1541, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b9f84a3500341e0b775a35da7904349"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0080, Acc: 0.9977, AUC: 0.9999, EER: 0.0035\nMemory usage after epoch 11:\nRAM Usage: 1692.96 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0035\n\nEpoch 12/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5852794ada5a4d0dbf5830caa5acc65b"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1510, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae21fec9a9a247ac9d8756bde63358ca"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0120, Acc: 0.9967, AUC: 0.9999, EER: 0.0047\nMemory usage after epoch 12:\nRAM Usage: 1693.09 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\n\nEpoch 13/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a0c6303fe34c2b9bb70ca1d1082129"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1472, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ab434b0d65464caef1da28cd711a8f"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0113, Acc: 0.9976, AUC: 0.9999, EER: 0.0049\nMemory usage after epoch 13:\nRAM Usage: 1693.09 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\n\nEpoch 14/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c68a5fadbf4a83a9e3b98db5a72ede"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1425, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e82d10a46b094088aa10641802c63f5a"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0089, Acc: 0.9977, AUC: 0.9999, EER: 0.0048\nMemory usage after epoch 14:\nRAM Usage: 1693.34 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\n\nEpoch 15/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa3beab219a844ea9def25e3a052098c"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1411, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ef145dca24418ebcb59cce9b9b7e8d"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0060, Acc: 0.9986, AUC: 1.0000, EER: 0.0028\nMemory usage after epoch 15:\nRAM Usage: 1693.34 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0028\n\nEpoch 16/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b48b22a6dc542418821c70082e4902d"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1362, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8242f6aa18c64866b74760edb0cd9223"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0081, Acc: 0.9984, AUC: 1.0000, EER: 0.0020\nMemory usage after epoch 16:\nRAM Usage: 1693.59 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0020\n\nEpoch 17/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806f93ea77b74f7c9f9b9813405ad38d"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1398, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"905d6f9cc6914b1195632707413e7cc0"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0065, Acc: 0.9983, AUC: 1.0000, EER: 0.0019\nMemory usage after epoch 17:\nRAM Usage: 1693.71 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0019\n\nEpoch 18/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032147cdd6094c1cbeac319bd1aeba36"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1447, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d6e8263a55249a593130064e5ee35bd"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0108, Acc: 0.9974, AUC: 1.0000, EER: 0.0019\nMemory usage after epoch 18:\nRAM Usage: 1693.71 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\n\nEpoch 19/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7570578508d404b9733a087f349de71"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1365, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec06b85c9f3c42c7869780726b9acb61"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0065, Acc: 0.9982, AUC: 1.0000, EER: 0.0018\nMemory usage after epoch 19:\nRAM Usage: 1693.84 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\nNew best model saved with EER: 0.0018\n\nEpoch 20/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3f30f7d4a8b4fd186fc3d5167205ad2"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1330, Acc: 0.0000, AUC: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ef75cb6bb643e181f35f883b116977"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.0051, Acc: 0.9986, AUC: 1.0000, EER: 0.0020\nMemory usage after epoch 20:\nRAM Usage: 1693.96 MB\nGPU Memory Usage: 23.42 MB / 16269.25 MB (0.1%)\n\nTraining completed!\nBest validation EER: 0.0018 at epoch 19\n\nEvaluating on evaluation set...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-37-a293fbf2af9f>:382: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(best_model_path)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluation:   0%|          | 0/2227 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0f496380dd048a0b2044ae9c75ade26"}},"metadata":{}},{"name":"stdout","text":"\nEvaluation Results:\nAccuracy: 0.6281\nAUC: 0.9285\nEER: 0.1562\nmin-tDCF: 0.5000\nEER Threshold: 0.9854\n\nFinal memory usage:\nRAM Usage: 1705.58 MB\nGPU Memory Usage: 24.78 MB / 16269.25 MB (0.2%)\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"def predict_audio(model, audio_path, device, sample_rate=16000, max_frames=64000, amp=True):\n    \"\"\"Predict whether an audio file is real or fake\"\"\"\n    # Load audio\n    audio, sr = torchaudio.load(audio_path)\n    \n    # Resample if needed\n    if sr != sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n        audio = resampler(audio)\n    \n    # Ensure mono\n    if audio.shape[0] > 1:\n        audio = torch.mean(audio, dim=0, keepdim=True)\n    \n    # Normalize\n    audio = audio / (torch.max(torch.abs(audio)) + 1e-8)\n    \n    # Pad or truncate\n    if audio.shape[1] < max_frames:\n        padding = max_frames - audio.shape[1]\n        audio = torch.nn.functional.pad(audio, (0, padding))\n    elif audio.shape[1] > max_frames:\n        audio = audio[:, :max_frames]\n    \n    # Add batch dimension - THIS LINE FIXES THE ISSUE\n    audio = audio.unsqueeze(0)\n    \n    # Move to device\n    audio = audio.to(device)\n    \n    # Set model to eval mode\n    model.eval()\n    \n    # Check if it's the fixed model type (without sigmoid in forward)\n    is_fixed_model = isinstance(model, AASIST_Fixed)\n    \n    # Get prediction\n    with torch.no_grad():\n        if amp and torch.cuda.is_available():\n            with torch.amp.autocast(device_type='cuda'):\n                output = model(audio)\n        else:\n            output = model(audio)\n    \n    # Apply sigmoid if using the fixed model\n    if is_fixed_model:\n        score = torch.sigmoid(output).item()\n    else:\n        score = output.item()\n    \n    # Return score\n    return score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:01:58.486080Z","iopub.execute_input":"2025-04-05T17:01:58.486367Z","iopub.status.idle":"2025-04-05T17:01:58.493421Z","shell.execute_reply.started":"2025-04-05T17:01:58.486346Z","shell.execute_reply":"2025-04-05T17:01:58.492698Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def visualize_waveform_and_spectrogram(audio_path, score, sample_rate=16000, threshold=0.5):\n    \"\"\"Visualize audio waveform and spectrogram with prediction\"\"\"\n    # Load audio\n    audio, sr = torchaudio.load(audio_path)\n    \n    # Resample if needed\n    if sr != sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n        audio = resampler(audio)\n    \n    # Ensure mono\n    if audio.shape[0] > 1:\n        audio = torch.mean(audio, dim=0, keepdim=True)\n    \n    # Convert to numpy\n    audio_np = audio.squeeze().numpy()\n    \n    # Create figure\n    plt.figure(figsize=(12, 10))\n    \n    # Plot waveform\n    plt.subplot(3, 1, 1)\n    time_axis = np.linspace(0, len(audio_np) / sample_rate, len(audio_np))\n    plt.plot(time_axis, audio_np)\n    plt.title('Audio Waveform')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    \n    # Plot spectrogram\n    plt.subplot(3, 1, 2)\n    D = librosa.stft(audio_np, n_fft=2048, hop_length=512)\n    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n    img = librosa.display.specshow(S_db, sr=sample_rate, hop_length=512, \n                          x_axis='time', y_axis='log', cmap='viridis')\n    plt.colorbar(img, format='%+2.0f dB')\n    plt.title('Spectrogram')\n    \n    # Add prediction visualization\n    plt.subplot(3, 1, 3)\n    \n    # Create a simple gauge chart for prediction\n    prediction = \"REAL\" if score > threshold else \"FAKE\"\n    confidence = score if score > threshold else 1 - score\n    \n    # Use different colors for real vs fake\n    color = 'green' if prediction == \"REAL\" else 'red'\n    \n    plt.barh(['Prediction'], [confidence], color=color)\n    plt.xlim(0, 1)\n    plt.xticks([0, 0.25, 0.5, 0.75, 1], ['0%', '25%', '50%', '75%', '100%'])\n    plt.title(f'Prediction: {prediction} (Score: {score:.4f}, Confidence: {confidence:.2%})')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:02:01.255149Z","iopub.execute_input":"2025-04-05T17:02:01.255460Z","iopub.status.idle":"2025-04-05T17:02:01.263159Z","shell.execute_reply.started":"2025-04-05T17:02:01.255435Z","shell.execute_reply":"2025-04-05T17:02:01.262261Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"def batch_evaluate_files(model, file_dir, device, sample_rate=16000, max_frames=64000, \n                         threshold=0.5, amp=True, ext='.flac'):\n    \"\"\"\n    Evaluate multiple audio files in a directory\n    \"\"\"\n    results = []\n    \n    # Find all audio files\n    files = [f for f in os.listdir(file_dir) if f.endswith(ext)]\n    print(f\"Found {len(files)} files to evaluate\")\n    \n    # Check if it's the fixed model type (without sigmoid in forward)\n    is_fixed_model = isinstance(model, AASIST_Fixed)\n    \n    # Process files\n    for file in tqdm(files, desc=\"Evaluating files\"):\n        file_path = os.path.join(file_dir, file)\n        \n        try:\n            # Load and preprocess audio\n            audio, sr = torchaudio.load(file_path)\n            \n            # Resample if needed\n            if sr != sample_rate:\n                resampler = torchaudio.transforms.Resample(sr, sample_rate)\n                audio = resampler(audio)\n            \n            # Ensure mono\n            if audio.shape[0] > 1:\n                audio = torch.mean(audio, dim=0, keepdim=True)\n            \n            # Normalize\n            audio = audio / (torch.max(torch.abs(audio)) + 1e-8)\n            \n            # Pad or truncate\n            if audio.shape[1] < max_frames:\n                padding = max_frames - audio.shape[1]\n                audio = torch.nn.functional.pad(audio, (0, padding))\n            elif audio.shape[1] > max_frames:\n                audio = audio[:, :max_frames]\n            \n            # Move to device\n            audio = audio.to(device)\n            \n            # Get prediction\n            model.eval()\n            with torch.no_grad():\n                if amp and torch.cuda.is_available():\n                    with torch.amp.autocast(device_type='cuda'):\n                        output = model(audio)\n                else:\n                    output = model(audio)\n            \n            # Apply sigmoid if using the fixed model\n            if is_fixed_model:\n                score = torch.sigmoid(output).item()\n            else:\n                score = output.item()\n            \n            # Determine prediction\n            prediction = \"REAL\" if score > threshold else \"FAKE\"\n            confidence = score if score > threshold else 1 - score\n            \n            # Store result\n            results.append({\n                'file': file,\n                'score': score,\n                'prediction': prediction,\n                'confidence': confidence\n            })\n        except Exception as e:\n            print(f\"Error processing {file}: {e}\")\n    \n    # Convert to dataframe\n    results_df = pd.DataFrame(results)\n    \n    # Display summary\n    print(\"\\nEvaluation Summary:\")\n    print(f\"Total files: {len(results_df)}\")\n    print(f\"Predicted as real: {len(results_df[results_df['prediction'] == 'REAL'])}\")\n    print(f\"Predicted as fake: {len(results_df[results_df['prediction'] == 'FAKE'])}\")\n    \n    # Save results\n    results_df.to_csv(os.path.join(file_dir, 'evaluation_results.csv'), index=False)\n    \n    return results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:02:04.329919Z","iopub.execute_input":"2025-04-05T17:02:04.330231Z","iopub.status.idle":"2025-04-05T17:02:04.339433Z","shell.execute_reply.started":"2025-04-05T17:02:04.330204Z","shell.execute_reply":"2025-04-05T17:02:04.338509Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Example of loading and using a saved model\ndef load_and_use_model(model_path, device):\n    \"\"\"Load a saved model and prepare it for inference\"\"\"\n    # Load checkpoint\n    checkpoint = torch.load(model_path, map_location=device)\n    \n    # Check if it's a fixed model\n    is_fixed_model = checkpoint.get('is_fixed_model', False)\n    \n    # Create appropriate model\n    if is_fixed_model:\n        print(\"Loading fixed model without sigmoid\")\n        model = AASIST_Fixed(\n            sinc_out_channels=config['model']['sinc_out_channels'],\n            sinc_kernel_size=config['model']['sinc_kernel_size'],\n            sample_rate=config['model']['sample_rate'],\n            res_channels=config['model']['res_channels'],\n            gat_nfeat=config['model']['gat_nfeat'],\n            gat_nhid=config['model']['gat_nhid'],\n            gat_nclass=config['model']['gat_nclass'],\n            gat_nheads=config['model']['gat_nheads'],\n            gat_alpha=config['model']['gat_alpha'],\n            gat_dropout=config['model']['gat_dropout'],\n            n_frame_node=config['model']['n_frame_node']\n        ).to(device)\n    else:\n        print(\"Loading original model with sigmoid\")\n        model = AASIST(\n            sinc_out_channels=config['model']['sinc_out_channels'],\n            sinc_kernel_size=config['model']['sinc_kernel_size'],\n            sample_rate=config['model']['sample_rate'],\n            res_channels=config['model']['res_channels'],\n            gat_nfeat=config['model']['gat_nfeat'],\n            gat_nhid=config['model']['gat_nhid'],\n            gat_nclass=config['model']['gat_nclass'],\n            gat_nheads=config['model']['gat_nheads'],\n            gat_alpha=config['model']['gat_alpha'],\n            gat_dropout=config['model']['gat_dropout'],\n            n_frame_node=config['model']['n_frame_node']\n        ).to(device)\n    \n    # Load state dict\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Set to evaluation mode\n    model.eval()\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:02:08.156785Z","iopub.execute_input":"2025-04-05T17:02:08.157086Z","iopub.status.idle":"2025-04-05T17:02:08.163534Z","shell.execute_reply.started":"2025-04-05T17:02:08.157063Z","shell.execute_reply":"2025-04-05T17:02:08.162656Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Load a trained model\nmodel_path = os.path.join(config['output_dir'], 'best_model.pth')\nmodel = load_and_use_model(model_path, device)\n\n# Predict on an audio file from your dataset\n# Using a real sample file from your dataset\naudio_path = \"/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_eval/flac/LA_E_1000147.flac\"\nprint(f\"Analyzing file: {audio_path}\")\nprint(f\"File exists: {os.path.exists(audio_path)}\")\n\n# Get prediction score\nscore = predict_audio(model, audio_path, device)\nprint(f\"Prediction score: {score:.4f} (Higher = more likely to be real)\")\n\n# Visualize the results\nvisualize_waveform_and_spectrogram(audio_path, score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:02:11.945445Z","iopub.execute_input":"2025-04-05T17:02:11.945767Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-57-4c844a43c8ab>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loading fixed model without sigmoid\nAnalyzing file: /kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_eval/flac/LA_E_1000147.flac\nFile exists: True\nPrediction score: 0.9702 (Higher = more likely to be real)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Example usage - uncomment to run\n# # Load a trained model\n# model_path = os.path.join(config['output_dir'], 'best_model.pth')\n# model = load_and_use_model(model_path, device)\n# \n# # Predict on an audio file\n# audio_path = \"/path/to/your/audio/file.flac\"\n# score = predict_audio(model, audio_path, device)\n# visualize_waveform_and_spectrogram(audio_path, score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:55:01.377893Z","iopub.execute_input":"2025-04-05T16:55:01.378094Z","iopub.status.idle":"2025-04-05T16:55:01.381081Z","shell.execute_reply.started":"2025-04-05T16:55:01.378077Z","shell.execute_reply":"2025-04-05T16:55:01.380286Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"# Conclusion and Further Improvements\n\n### The AASIST implementation offers state-of-the-art performance for detecting AI-generated human speech. Here are some possible improvements for future work:\n\n### 1. **Model Optimizations**:\n    - Model quantization for faster inference\n    - Knowledge distillation for smaller models\n    - Adaptive frame selection for variable-length audio\n \n### 2. **Training Enhancements**:\n    - Adversarial training for improved robustness\n    - Transfer learning from larger datasets\n    - Multi-task learning (combining spoofing detection with other tasks)\n \n### 3. **Real-world Applications**:\n    - Integration with web services and APIs\n    - Browser extensions for real-time detection\n    - Mobile applications for on-device detection","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}